# -*- coding: utf-8 -*-
"""Eval Notebook .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mY_UOjVa67IzHBT_02xcXxkZhUAGJs8h
"""

# Commented out IPython magic to ensure Python compatibility.

# %pip install datasets accelerate

text = """
33. DNA-LLM/virus_pythia_14_2048_2d_representation_MSEPlusCE
34. DNA-LLM/virus_pythia_14_2048_2d_representation_GaussianPlusCE
55. DNA-LLM/virus_pythia_14_2048_headless
56. DNA-LLM/virus_pythia_14_2048_cross_entropy
57. DNA-LLM/virus_pythia_14_2048_compliment"""

# !huggingface-cli login

text = text.split("\n")
text_new = [text.split('.') for text in text[1:]]
text_new_2 = [text[1] for text in text_new]
text_new_3 = [text.strip() for text in text_new_2]

text_new_3

from transformers import AutoModel, AutoModelForCausalLM
import pandas as pd

def load_and_save_model_params(repo_ids):
    repo_id_list = []
    model_params_list = []

    for repo_id in repo_ids:
      try:
        # Load the model from the Hugging Face repository
        model = AutoModelForCausalLM.from_pretrained(repo_id, trust_remote_code= True)

        # Get the number of model parameters
        num_params = model.num_parameters()

        # Append the repository ID and model parameters to the lists
        repo_id_list.append(repo_id)
        model_params_list.append(num_params)
      except:
        repo_id_list.append(repo_id)
        model_params_list.append('Unknown')

    # Create a DataFrame using the lists
    df = pd.DataFrame({'Repository ID': repo_id_list, 'Model Parameters': model_params_list})

    # Save the DataFrame to a CSV file
    csv_path = "model_params.csv"
    df.to_csv(csv_path, index=False)
    print(f"Model parameters and repository IDs saved to: {csv_path}")


load_and_save_model_params(text_new_3)



# Commented out IPython magic to ensure Python compatibility.
# %pip install einops flash_attn

## %pip install datasets # %pip install accelerate  -- for colab
import logging
import torch
from datasets import load_dataset
from transformers import TrainingArguments, AutoTokenizer, AutoConfig, AutoModelWithLMHead


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

## Standard CE Loss ###
import torch
from transformers import Trainer

class StandardLoss(Trainer):
    def compute_loss(self, model, inputs):
        input_ids = inputs.pop("input_ids")
        labels = input_ids.clone().to(model.device)
        input_ids = input_ids[:, :-1].contiguous()  # Truncate the input_ids

        lm_logits = model(input_ids).logits
        shift_labels = labels[:, 1:].contiguous()

        loss_fct = torch.nn.CrossEntropyLoss()
        lm_loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), shift_labels.view(-1))

        return lm_loss

# !huggingface-cli login

from datasets import load_dataset

#### Datasets ####

ds_train = load_dataset('Hack90/experiment_one_viral_genomes_train_set_v2')
ds_valid = load_dataset('Hack90/experiment_one_viral_genomes_val_set_v2')
ds_test = load_dataset('Hack90/experiment_one_viral_genomes_test_set_v2')

#### Models ####
from transformers import TrainingArguments, AutoTokenizer, AutoConfig, AutoModelWithLMHead, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Hack90/virus_pythia_31_2048")

def get_model(repo_id):
  model = AutoModelForCausalLM.from_pretrained(repo_id, trust_remote_code=True)
  return model

text_new_3

models = [get_model(repo_id) for repo_id in text_new_3]

##### Finetune ####
PARAMS = '14'
MAX_LENGTH = 2048
BATCH_SIZE = 72


for model,repo_id in zip(models[1:],text_new_3[1:]):
  training_args = TrainingArguments(
      output_dir=f"./virus_pythia_{repo_id}",
      num_train_epochs=0.2,
      per_device_train_batch_size=BATCH_SIZE,
      per_device_eval_batch_size=BATCH_SIZE,
      warmup_steps=10,
      logging_steps=10,
      logging_dir="./logs",
      #fp16=True,
      learning_rate=0.00005,
      dataloader_num_workers=4,
      dataloader_prefetch_factor=2,
  )

  trainer = StandardLoss(
      model=model,
      args=training_args,
      train_dataset=ds_valid['train'],
      eval_dataset=ds_test['train'],
      tokenizer=tokenizer,
  )

  logger.info("Training the model...")
  trainer.train()

  logger.info("Evaluating the model against the test set...")
  #eval_results = trainer.evaluate(eval_dataset=ds_test['train'])
  #logger.info(f"Evaluation results: {eval_results}")

import pandas as pd

df = pd.read_csv('/content/Pythia 14M Fintune - Sheet1.csv')

